[
    {
        "id": 1,
        "name": "Isolation Forest",
        "status": "active",
        "code_snippet": "def train_isolation_forest_hypertuned(X_train, y_train, X_test, y_test, logger):\n    \"\"\"Train an isolation forest model with hyperparameter tuning\"\"\"\n    # Define parameter grid\n    param_grid = {\n        'n_estimators': [100, 200, 300],\n        'contamination': [0.1, 0.2, 0.3, 0.4, 0.5],\n        'max_samples': ['auto', 256, 512],\n        'max_features': [0.5, 0.75, 1.0],\n        'bootstrap': [True, False],\n        'n_jobs': [-1]\n    }\n    \n    # Random search for hyperparameter tuning\n    random_search = RandomizedSearchCV(\n        IsolationForest(random_state=42),\n        param_distributions=param_grid,\n        n_iter=20,\n        scoring=make_scorer(custom_isolation_forest_scorer, greater_is_better=True),\n        cv=3,\n        verbose=1,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    # Fit and return best model\n    random_search.fit(X_train)\n    return random_search.best_estimator_, random_search.best_params_"
    },
    {
        "id": 2,
        "name": "LSTM Autoencoder",
        "status": "inactive",
        "code_snippet": "def create_lstm_autoencoder_model(input_dim, hyperparams):\n    \"\"\"Create an LSTM autoencoder for anomaly detection\"\"\"\n    model = Sequential()\n    \n    # Encoder\n    model.add(LSTM(\n        hyperparams['encoder_units'], \n        activation=hyperparams['activation'],\n        input_shape=(1, input_dim), \n        return_sequences=False\n    ))\n    model.add(Dropout(hyperparams['dropout_rate']))\n    model.add(Dense(hyperparams['latent_dim'], activation=hyperparams['activation']))\n    model.add(Dropout(hyperparams['dropout_rate']))\n    \n    # Decoder\n    model.add(RepeatVector(1))\n    model.add(LSTM(\n        hyperparams['decoder_units'], \n        activation=hyperparams['activation'], \n        return_sequences=True\n    ))\n    model.add(Dropout(hyperparams['dropout_rate']))\n    model.add(TimeDistributed(Dense(input_dim)))\n    \n    model.compile(\n        optimizer=Adam(learning_rate=hyperparams['learning_rate']),\n        loss=hyperparams['loss_function']\n    )\n    \n    return model"
    },
    {
        "id": 3,
        "name": "Hybrid Ensemble Model",
        "status": "inactive",
        "code_snippet": "def train_models(X, y, logger, device_type='auto', hypertune=True):\n    \"\"\"Train the hybrid ensemble of models\"\"\"\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Initialize hybrid detector\n    detector = HybridIoTMalwareDetector(device_type=device_type)\n    \n    if hypertune:\n        # Train base models\n        detector.train_batch_models(X_train, y_train, X_test, y_test, skip_hypertuning=True)\n        \n        # Hypertune Isolation Forest\n        best_if_model, best_if_params = train_isolation_forest_hypertuned(\n            X_train, y_train, X_test, y_test, logger\n        )\n        detector.models['isolation_forest_tuned'] = best_if_model\n        \n        # Hypertune LSTM Autoencoder\n        best_lstm_model, model_info = train_lstm_autoencoder_hypertuned(\n            X_train, y_train, X_test, y_test, logger, device_type\n        )\n        detector.models['lstm_autoencoder_tuned'] = best_lstm_model\n    else:\n        # Train without hypertuning\n        detector.train_batch_models(X_train, y_train, X_test, y_test)\n    \n    return detector"
    },
    {
        "id": 4,
        "name": "LSTM Autoencoder Training with Hyperparameter Tuning",
        "status": "inactive",
        "code_snippet": "def train_lstm_autoencoder_hypertuned(X_train, y_train, X_test, y_test, logger, device_type='auto'):\n    \"\"\"Train LSTM autoencoder with hyperparameter tuning\"\"\"\n    # Hyperparameter combinations\n    hyperparams_list = [\n        {\n            'encoder_units': 128,\n            'decoder_units': 128,\n            'latent_dim': 64,\n            'dropout_rate': 0.2,\n            'learning_rate': 0.001,\n            'batch_size': 64,\n            'activation': 'relu',\n            'loss_function': 'mse'\n        },\n        {\n            'encoder_units': 256,\n            'decoder_units': 256,\n            'latent_dim': 128,\n            'dropout_rate': 0.3,\n            'learning_rate': 0.0005,\n            'batch_size': 128,\n            'activation': 'tanh',\n            'loss_function': 'mae'\n        },\n        {\n            'encoder_units': 64,\n            'decoder_units': 64,\n            'latent_dim': 32,\n            'dropout_rate': 0.1,\n            'learning_rate': 0.002,\n            'batch_size': 32,\n            'activation': 'relu',\n            'loss_function': 'mse'\n        }\n    ]\n    \n    # Training callbacks\n    callbacks = [\n        EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=0.00001,\n            verbose=1\n        )\n    ]\n    \n    # Train and evaluate models with different hyperparameters\n    for hyperparams in hyperparams_list:\n        model = create_lstm_autoencoder_model(X_train.shape[1], hyperparams)\n        model.fit(\n            X_train_normal, X_train_normal,\n            epochs=15,\n            batch_size=hyperparams['batch_size'],\n            validation_data=(X_test_normal, X_test_normal),\n            callbacks=callbacks,\n            verbose=1\n        )\n    \n    return best_model, model_info"
    }
]